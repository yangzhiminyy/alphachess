![chess board](/screenshots/board.png)

# AlphaChess - Chinese Chess AI with AlphaZero

[English](#english) | [ä¸­æ–‡](#chinese)

---

<a name="english"></a>
## English Documentation

### Project Overview

**AlphaChess** is a complete implementation of an AlphaZero-style reinforcement learning system for Chinese Chess (Xiangqi). This project demonstrates advanced AI techniques including Monte Carlo Tree Search (MCTS), deep neural networks, and self-play training, all wrapped in a modern web application with REST API.

**Key Features:**
- ğŸ® Full Xiangqi game engine with strict rule enforcement
- ğŸ§  AlphaZero-style neural network (Policy-Value Network)
- ğŸŒ² Monte Carlo Tree Search (MCTS) with PUCT algorithm
- ğŸ” Alpha-Beta search with advanced optimizations
- ğŸ¯ Self-play training pipeline with JSONL data format
- ğŸ† Arena/ELO evaluation system for model comparison
- ğŸŒ Modern web interface with React
- ğŸ”Œ RESTful API with FastAPI
- ğŸ¨ Model visualization and inspection tools
- ğŸ”„ Generic framework extensible to other board games
- â˜ï¸ Azure cloud deployment with automated scripts

### Technology Stack

**Backend:**
- Python 3.10+
- PyTorch (Deep Learning)
- FastAPI (REST API)
- NumPy (Numerical Computing)

**Frontend:**
- React 18
- Babel (JSX Compilation)
- Modern CSS3

**Architecture:**
- Model-View-Controller (MVC)
- Abstract interfaces for game-agnostic design
- Modular architecture for easy extension

### Project Structure

```
alphachess/
â”œâ”€â”€ xq/                          # Xiangqi game engine
â”‚   â”œâ”€â”€ constants.py            # Board dimensions, piece types
â”‚   â”œâ”€â”€ move.py                 # 32-bit move encoding
â”‚   â”œâ”€â”€ zobrist.py              # Zobrist hashing for positions
â”‚   â”œâ”€â”€ state.py                # Core game logic (800+ lines)
â”‚   â”œâ”€â”€ policy.py               # Move indexing for neural network
â”‚   â”œâ”€â”€ nn.py                   # Legacy neural network
â”‚   â”œâ”€â”€ mcts.py                 # MCTS implementation
â”‚   â”œâ”€â”€ selfplay.py             # Self-play data generation
â”‚   â”œâ”€â”€ game_adapter.py         # GameInterface adapter
â”‚   â””â”€â”€ search/
â”‚       â””â”€â”€ alpha_beta.py       # Alpha-Beta with TT, QS
â”‚
â”œâ”€â”€ alphazero/                   # Generic AlphaZero framework
â”‚   â”œâ”€â”€ game_interface.py       # Abstract game interface
â”‚   â”œâ”€â”€ network.py              # Generic PolicyValueNet
â”‚   â”œâ”€â”€ mcts_generic.py         # Game-agnostic MCTS
â”‚   â””â”€â”€ trainer.py              # Generic training pipeline
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ server.py               # FastAPI REST API (770+ lines)
â”‚
â”œâ”€â”€ web/                         # React frontend
â”‚   â”œâ”€â”€ index.html              # Main game interface
â”‚   â”œâ”€â”€ app.js                  # Game UI logic
â”‚   â”œâ”€â”€ arena.html              # Arena evaluation
â”‚   â”œâ”€â”€ arena.js                # Arena UI logic
â”‚   â”œâ”€â”€ model.html              # Model viewer
â”‚   â””â”€â”€ model.js                # Model inspection
â”‚
â”œâ”€â”€ scripts/                     # CLI tools
â”‚   â”œâ”€â”€ train.py                # Legacy training
â”‚   â”œâ”€â”€ train_generic.py        # Generic training
â”‚   â”œâ”€â”€ self_play.py            # Legacy self-play
â”‚   â”œâ”€â”€ self_play_generic.py    # Generic self-play
â”‚   â”œâ”€â”€ arena.py                # ELO evaluation
â”‚   â””â”€â”€ test_integration.py     # Integration tests
â”‚
â””â”€â”€ docs/                        # Documentation
    â”œâ”€â”€ README_FRAMEWORK.md     # Architecture details
    â”œâ”€â”€ MIGRATION_GUIDE.md      # Migration guide
    â”œâ”€â”€ VERSION_COMPARISON.md   # Version comparison
    â”œâ”€â”€ ARENA_GUIDE.md          # Arena usage guide
    â””â”€â”€ CHANGELOG.md            # Version history
```

### Core Components

#### 1. Game Engine (`xq/state.py`)

The heart of the system, implementing complete Xiangqi rules:

**Key Features:**
- 9Ã—10 board representation with signed integers
- Incremental move application with undo support
- Zobrist hashing for fast position lookup
- Pseudo-legal and legal move generation
- Check detection and checkmate adjudication
- Threefold repetition detection
- Chinese-specific rules:
  - Palace constraints for King and Advisors
  - River crossing for Pawns
  - Cannon jump mechanics
  - Knight blocking detection
  - General facing rule
  - Perpetual check/chase detection

**Performance:**
- ~44 legal moves from starting position
- Sub-millisecond move generation
- Zobrist hash for O(1) position lookup

#### 2. Neural Network Architecture

**Input Representation:**
- 15 channels Ã— 10 rows Ã— 9 columns
- Channels: 14 piece types (7 pieces Ã— 2 colors) + 1 side-to-move

**Network Architecture (Generic):**
```
Input (15, 10, 9)
    â†“
Conv2d(3Ã—3) + BatchNorm + ReLU
    â†“
N Ã— Residual Blocks
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Policy Head  â”‚  Value Head  â”‚
â”‚              â”‚              â”‚
â”‚ Conv(1Ã—1)    â”‚  Conv(1Ã—1)   â”‚
â”‚ + BatchNorm  â”‚  + BatchNorm â”‚
â”‚ + Flatten    â”‚  + Flatten   â”‚
â”‚ + Linear     â”‚  + Linear    â”‚
â”‚              â”‚  + Tanh      â”‚
â”‚ (8100 dims)  â”‚  (scalar)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Parameters:**
- `hidden_channels`: 64 (default), 128 (advanced)
- `num_res_blocks`: 3 (default), 7-10 (advanced)
- Action space: 8100 (90Ã—90 from-to moves)
- Value range: [-1, 1] (loss to win)

**Network Types:**
1. **Legacy XQNet**: Simple CNN without BatchNorm
2. **Generic PolicyValueNet**: Modern architecture with BatchNorm

#### 3. Search Algorithms

##### Monte Carlo Tree Search (MCTS)

**Algorithm:** PUCT (Predictor + Upper Confidence Bound for Trees)

**Formula:**
$$ UCT = Q(s,a) + c\_puct Ã— P(s,a) Ã— \frac{\sqrt{N(s)} }{(1 + N(s,a))} $$

**Features:**
- Dirichlet noise for exploration (Î±=0.3)
- Temperature parameter for action selection
- Virtual loss for parallel search
- Legal move masking

**Parameters:**
- `num_simulations`: 100-800 (default: 200)
- `c_puct`: 1.5 (exploration constant)
- `temperature`: 1.0 (exploration) â†’ 0.1 (exploitation)

##### Alpha-Beta Search

**Features:**
- Negamax framework
- Transposition Table (TT) with Zobrist hashing
- Quiescence search for tactical stability
- Move ordering:
  - MVV/LVA (Most Valuable Victim / Least Valuable Attacker)
  - History heuristic
  - Killer moves

**Parameters:**
- `depth`: 3-6 (default: 3)
- `tt_size_mb`: 256 (transposition table size)

#### 4. Training Pipeline

**Self-Play Process:**
```
1. Initialize game from starting position
2. For each move:
   - Run MCTS (with Dirichlet noise)
   - Sample action based on visit counts
   - Apply move to board
3. Record (state, policy, value) tuples
4. Continue until game ends
5. Assign final outcome to all positions
6. Save to JSONL format
```

**Training Process:**
```
1. Load self-play data from JSONL
2. Create PyTorch DataLoader
3. For each epoch:
   - Forward pass: predict policy and value
   - Loss = CrossEntropy(policy) + MSE(value)
   - Backward pass and optimize
4. Save checkpoint
```

**Data Format (JSONL):**
```json
{
  "game_id": 0,
  "result": 1,
  "records": [
    {
      "planes": [[...], ...],  // 15Ã—90 board state
      "pi": {"123": 0.15, ...}, // Policy distribution
      "z": 1.0                  // Game outcome
    }
  ],
  "timestamp": "2025-11-05T..."
}
```

### REST API Reference

Base URL: `http://127.0.0.1:8000/api`

#### Game Management

**Create Game**
```
POST /games
Body: {
  "squares": [int] (optional),
  "side_to_move": int (optional)
}
Response: {
  "game_id": "uuid",
  "state": {...}
}
```

**Get Game State**
```
GET /games/{game_id}
Response: {
  "board": [int],
  "side_to_move": int,
  "legal_moves": [...]
}
```

**Make Move**
```
POST /games/{game_id}/move
Body: {
  "from_sq": int,
  "to_sq": int
}
```

**Undo Move**
```
POST /games/{game_id}/undo
```

#### AI Features

**Get Legal Moves**
```
GET /games/{game_id}/legal_moves
Response: [
  {"from": 81, "to": 72, "move_id": 123, ...}
]
```

**Get Best Move**
```
POST /games/{game_id}/best
Body: {
  "engine": "alphabeta" | "mcts" | "mcts_nn",
  "depth": int (for alphabeta),
  "sims": int (for mcts),
  "model_path": string (for mcts_nn)
}
Response: {
  "best": {"from": 81, "to": 72},
  "score": float,
  "pi": {...}
}
```

**Human-AI Play**
```
POST /games/{game_id}/human_ai
Body: {
  "human_move": "a0-a1",
  "engine": "mcts_nn",
  "sims": 200
}
Response: {
  "human": {...},
  "ai": {...},
  "state": {...}
}
```

#### Model Management

**List Models**
```
GET /model/list?dir=models
Response: {
  "models": ["latest.pt", ...]
}
```

**Get Model Info**
```
GET /model/info?model_path=models/latest.pt
Response: {
  "path": "...",
  "size_mb": 2.5,
  "parameters": 150000,
  "structure": [...]
}
```

**Get Model Framework**
```
GET /model/framework
Response: {
  "loaded": true,
  "path": "models/latest.pt",
  "framework": "generic" | "legacy"
}
```

#### Training & Evaluation

**Self-Play**
```
POST /selfplay
Body: {
  "engine": "mcts" | "mcts_nn",
  "games": 1,
  "sims": 200,
  "max_moves": 200
}
```

**Arena Evaluation**
```
POST /arena/run
Body: {
  "engine_a": "mcts_nn",
  "engine_b": "alphabeta",
  "model_a": "models/v1.pt",
  "params_a": {"sims": 200},
  "params_b": {"depth": 3},
  "n_games": 20
}
Response: {
  "elo_diff": 121.3,
  "win_rate": 0.675,
  "wins": 12,
  "draws": 3,
  "losses": 5
}
```

### Installation & Setup

#### Local Installation

**Prerequisites:**
```bash
# Python 3.10 or higher
python --version

# pip package manager
pip --version
```

**Install Dependencies:**
```bash
# Core dependencies
pip install torch torchvision  # PyTorch
pip install fastapi uvicorn    # API server
pip install numpy              # Numerical computing

# Optional: for GPU support
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

**Quick Start:**

1. **Clone or download the project**

2. **Start the API server:**
```bash
uvicorn api.server:app --host 127.0.0.1 --port 8000
```

3. **Access the web interface:**
- Main game: http://127.0.0.1:8000/web/
- Model viewer: http://127.0.0.1:8000/web/model.html
- Arena: http://127.0.0.1:8000/web/arena.html

#### â˜ï¸ Azure Cloud Deployment

For production deployment with GPU training on Azure cloud:

**Quick Deploy (10 minutes):**
```bash
cd deployment/azure/scripts
bash deploy_to_azure.sh
```

**Documentation:**
- ğŸ“– [Azure Quick Start Guide](deployment/azure/docs/AZURE_QUICKSTART.md) - Start here!
- ğŸ“– [Complete Deployment Guide](deployment/azure/docs/azure_deployment_guide.md) - Full details
- ğŸ“‹ [Deployment Checklist](deployment/azure/docs/deployment_checklist.md) - Step-by-step
- ğŸ’° Cost: ~$50/month (includes GPU training)

**Features:**
- One-click deployment to Azure App Service
- Automated GPU training on Azure VM
- Blob Storage for models and data
- Auto-scaling and monitoring
- Budget-friendly ($150/month plan included)

### Usage Examples

#### Example 1: Self-Play Training

```bash
# Step 1: Generate self-play data
python scripts/self_play_generic.py \
    --game xiangqi \
    --games 50 \
    --sims 200 \
    --out data/selfplay_001.jsonl

# Step 2: Train the model
python scripts/train_generic.py \
    --game xiangqi \
    --data data/selfplay_001.jsonl \
    --model_out models/v1.pt \
    --epochs 10 \
    --batch_size 64 \
    --lr 0.001

# Step 3: Repeat with new model
python scripts/self_play_generic.py \
    --game xiangqi \
    --games 50 \
    --sims 200 \
    --model models/v1.pt \
    --out data/selfplay_002.jsonl
```

#### Example 2: Model Evaluation

```bash
# Compare two models
python scripts/arena.py \
    --engine-a mcts_nn --model-a models/v1.pt \
    --engine-b mcts_nn --model-b models/v2.pt \
    --games 20 \
    --output results.json

# Test against Alpha-Beta baseline
python scripts/arena.py \
    --engine-a mcts_nn --model-a models/latest.pt \
    --engine-b alphabeta --depth-b 3 \
    --games 20
```

#### Example 3: Human Play via API

```python
import requests

# Create game
response = requests.post('http://127.0.0.1:8000/api/games')
game_id = response.json()['game_id']

# Make human move and get AI response
response = requests.post(
    f'http://127.0.0.1:8000/api/games/{game_id}/human_ai',
    json={
        'human_move': 'b9-c7',  # Move knight
        'engine': 'mcts_nn',
        'sims': 200
    }
)

ai_move = response.json()['ai']
print(f"AI plays: {ai_move['from_coord']} â†’ {ai_move['to_coord']}")
```

### Key Algorithms

#### 1. Move Generation (Pseudo-code)

```python
def generate_legal_moves(state):
    pseudo_legal = []
    
    # For each piece of current player
    for square, piece in enumerate(state.board):
        if piece.color != state.side_to_move:
            continue
            
        # Generate piece-specific moves
        if piece.type == PAWN:
            moves = generate_pawn_moves(square, piece)
        elif piece.type == CANNON:
            moves = generate_cannon_moves(square, piece)
        # ... other pieces
        
        pseudo_legal.extend(moves)
    
    # Filter illegal moves (king in check)
    legal = []
    for move in pseudo_legal:
        state.apply_move(move)
        if not state.is_in_check(opponent):
            legal.append(move)
        state.undo_move()
    
    return legal
```

#### 2. MCTS Selection (Pseudo-code)

```python
def select(node, state):
    while not node.is_leaf():
        # PUCT formula
        best_action = None
        best_value = -inf
        
        for action, child in node.children.items():
            q = child.value()  # Average value
            u = c_puct * child.prior * sqrt(node.visits) / (1 + child.visits)
            value = q + u
            
            if value > best_value:
                best_value = value
                best_action = action
        
        state.apply_move(best_action)
        node = node.children[best_action]
    
    return node, state
```

#### 3. Alpha-Beta Search (Pseudo-code)

```python
def alphabeta(state, depth, alpha, beta):
    # Terminal condition
    if depth == 0 or state.is_terminal():
        return evaluate(state)
    
    # Check transposition table
    tt_entry = tt.lookup(state.hash)
    if tt_entry and tt_entry.depth >= depth:
        return tt_entry.score
    
    # Move ordering
    moves = state.generate_legal_moves()
    moves = order_moves(moves, state)  # MVV/LVA, history, killer
    
    best_score = -infinity
    for move in moves:
        state.apply_move(move)
        score = -alphabeta(state, depth-1, -beta, -alpha)
        state.undo_move()
        
        best_score = max(best_score, score)
        alpha = max(alpha, score)
        
        if alpha >= beta:  # Beta cutoff
            record_killer(move)
            break
    
    # Store in transposition table
    tt.store(state.hash, depth, best_score)
    
    return best_score
```

### Performance Benchmarks

**Hardware:** Intel i7-10700K, 16GB RAM, CPU only

| Operation | Time | Notes |
|-----------|------|-------|
| Move Generation | < 1 ms | ~44 legal moves from start |
| Position Evaluation | < 0.1 ms | Material + basic heuristics |
| Alpha-Beta (depth=3) | ~2-5 s | With TT and move ordering |
| MCTS (200 sims) | ~5-10 s | With simple policy |
| MCTS+NN (200 sims) | ~15-30 s | With neural network |
| NN Inference | ~10 ms | Single position, CPU |
| Self-Play Game | ~2-5 min | MCTS, 200 sims, ~100 moves |

### Testing

Run integration tests:
```bash
python scripts/test_integration.py
```

**Test Coverage:**
- âœ… Import tests (legacy + generic)
- âœ… Legacy framework (XQNet, state, moves)
- âœ… Generic framework (GameInterface, MCTS, Trainer)
- âœ… Model compatibility (save/load)
- âœ… GameInterface implementation

### Extending to Other Games

The generic framework makes it easy to add new games:

```python
# games/go/game_adapter.py
from alphazero import GameInterface

class GoGame(GameInterface):
    def get_initial_state(self):
        return empty_19x19_board()
    
    def get_action_size(self):
        return 19 * 19 + 1  # +1 for pass
    
    def get_observation_shape(self):
        return (17, 19, 19)  # 8 history Ã— 2 colors + side
    
    # Implement other abstract methods...

# Use with existing framework
game = GoGame()
model = PolicyValueNet(NetworkConfig(
    input_channels=17,
    board_height=19,
    board_width=19,
    action_size=362
))
```

### Future Enhancements

- [ ] GPU acceleration and batch inference
- [ ] Distributed self-play across multiple machines
- [ ] Opening book integration
- [ ] Endgame tablebase
- [ ] ELO rating system with SPRT
- [ ] Multi-game training (transfer learning)
- [ ] Mobile app (React Native)
- [ ] Tournament mode with Swiss pairing

### Technical Highlights (for Job Applications)

**Demonstrated Skills:**

1. **Deep Learning**
   - PyTorch neural network design and training
   - Policy-Value Network architecture
   - Residual connections and BatchNorm

2. **Reinforcement Learning**
   - AlphaZero algorithm implementation
   - Monte Carlo Tree Search
   - Self-play training pipeline

3. **Software Engineering**
   - Clean architecture with abstract interfaces
   - Modular design for extensibility
   - Comprehensive documentation

4. **Backend Development**
   - RESTful API with FastAPI
   - Asynchronous request handling
   - State management

5. **Frontend Development**
   - React components and hooks
   - Interactive game interface
   - Data visualization

6. **Algorithms**
   - Game tree search (Alpha-Beta, MCTS)
   - Heuristic optimization (TT, QS, move ordering)
   - Complex rule enforcement

7. **Testing & Quality**
   - Integration test suite
   - Model versioning and evaluation
   - Performance benchmarking

### License

This project is for educational and portfolio purposes.

### Contact

For job opportunities or technical discussions, please contact via GitHub.

---

<a name="chinese"></a>
## ä¸­æ–‡æ–‡æ¡£

### é¡¹ç›®æ¦‚è¿°

**AlphaChess** æ˜¯ä¸€ä¸ªå®Œæ•´å®ç°çš„ AlphaZero é£æ ¼çš„ä¸­å›½è±¡æ£‹å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿã€‚è¯¥é¡¹ç›®å±•ç¤ºäº†å…ˆè¿›çš„ AI æŠ€æœ¯ï¼ŒåŒ…æ‹¬è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ã€æ·±åº¦ç¥ç»ç½‘ç»œå’Œè‡ªå¯¹å¼ˆè®­ç»ƒï¼Œå¹¶é…æœ‰ç°ä»£åŒ–çš„ Web åº”ç”¨å’Œ REST APIã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ğŸ® å®Œæ•´çš„è±¡æ£‹å¼•æ“ï¼Œä¸¥æ ¼æ‰§è¡Œè§„åˆ™
- ğŸ§  AlphaZero é£æ ¼çš„ç¥ç»ç½‘ç»œï¼ˆç­–ç•¥-ä»·å€¼ç½‘ç»œï¼‰
- ğŸŒ² ä½¿ç”¨ PUCT ç®—æ³•çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢
- ğŸ” å¸¦é«˜çº§ä¼˜åŒ–çš„ Alpha-Beta æœç´¢
- ğŸ¯ è‡ªå¯¹å¼ˆè®­ç»ƒæµç¨‹ï¼Œä½¿ç”¨ JSONL æ•°æ®æ ¼å¼
- ğŸ† ç«æŠ€åœº/ELO è¯„æµ‹ç³»ç»Ÿç”¨äºæ¨¡å‹æ¯”è¾ƒ
- ğŸŒ åŸºäº React çš„ç°ä»£ Web ç•Œé¢
- ğŸ”Œ åŸºäº FastAPI çš„ RESTful API
- ğŸ¨ æ¨¡å‹å¯è§†åŒ–å’Œæ£€æŸ¥å·¥å…·
- ğŸ”„ å¯æ‰©å±•åˆ°å…¶ä»–æ£‹ç±»æ¸¸æˆçš„é€šç”¨æ¡†æ¶
- â˜ï¸ Azure äº‘éƒ¨ç½²ï¼Œé…æœ‰è‡ªåŠ¨åŒ–è„šæœ¬

### æŠ€æœ¯æ ˆ

**åç«¯ï¼š**
- Python 3.10+
- PyTorchï¼ˆæ·±åº¦å­¦ä¹ ï¼‰
- FastAPIï¼ˆREST APIï¼‰
- NumPyï¼ˆæ•°å€¼è®¡ç®—ï¼‰

**å‰ç«¯ï¼š**
- React 18
- Babelï¼ˆJSX ç¼–è¯‘ï¼‰
- ç°ä»£ CSS3

**æ¶æ„ï¼š**
- MVCï¼ˆModel-View-Controllerï¼‰
- æ¸¸æˆæ— å…³çš„æŠ½è±¡æ¥å£è®¾è®¡
- æ¨¡å—åŒ–æ¶æ„ï¼Œæ˜“äºæ‰©å±•

### é¡¹ç›®ç»“æ„

```
alphachess/
â”œâ”€â”€ xq/                          # è±¡æ£‹æ¸¸æˆå¼•æ“
â”‚   â”œâ”€â”€ constants.py            # æ£‹ç›˜å°ºå¯¸ã€æ£‹å­ç±»å‹
â”‚   â”œâ”€â”€ move.py                 # 32ä½èµ°æ³•ç¼–ç 
â”‚   â”œâ”€â”€ zobrist.py              # å±€é¢çš„ Zobrist å“ˆå¸Œ
â”‚   â”œâ”€â”€ state.py                # æ ¸å¿ƒæ¸¸æˆé€»è¾‘ï¼ˆ800+ è¡Œï¼‰
â”‚   â”œâ”€â”€ policy.py               # ç¥ç»ç½‘ç»œçš„èµ°æ³•ç´¢å¼•
â”‚   â”œâ”€â”€ nn.py                   # ä¼ ç»Ÿç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ mcts.py                 # MCTS å®ç°
â”‚   â”œâ”€â”€ selfplay.py             # è‡ªå¯¹å¼ˆæ•°æ®ç”Ÿæˆ
â”‚   â”œâ”€â”€ game_adapter.py         # GameInterface é€‚é…å™¨
â”‚   â””â”€â”€ search/
â”‚       â””â”€â”€ alpha_beta.py       # Alpha-Beta æœç´¢ï¼ˆTTã€QSï¼‰
â”‚
â”œâ”€â”€ alphazero/                   # é€šç”¨ AlphaZero æ¡†æ¶
â”‚   â”œâ”€â”€ game_interface.py       # æŠ½è±¡æ¸¸æˆæ¥å£
â”‚   â”œâ”€â”€ network.py              # é€šç”¨ PolicyValueNet
â”‚   â”œâ”€â”€ mcts_generic.py         # æ¸¸æˆæ— å…³çš„ MCTS
â”‚   â””â”€â”€ trainer.py              # é€šç”¨è®­ç»ƒæµç¨‹
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ server.py               # FastAPI REST APIï¼ˆ770+ è¡Œï¼‰
â”‚
â”œâ”€â”€ web/                         # React å‰ç«¯
â”‚   â”œâ”€â”€ index.html              # ä¸»æ¸¸æˆç•Œé¢
â”‚   â”œâ”€â”€ app.js                  # æ¸¸æˆ UI é€»è¾‘
â”‚   â”œâ”€â”€ arena.html              # ç«æŠ€åœºè¯„æµ‹
â”‚   â”œâ”€â”€ arena.js                # ç«æŠ€åœº UI é€»è¾‘
â”‚   â”œâ”€â”€ model.html              # æ¨¡å‹æŸ¥çœ‹å™¨
â”‚   â””â”€â”€ model.js                # æ¨¡å‹æ£€æŸ¥
â”‚
â”œâ”€â”€ scripts/                     # CLI å·¥å…·
â”‚   â”œâ”€â”€ train.py                # ä¼ ç»Ÿè®­ç»ƒ
â”‚   â”œâ”€â”€ train_generic.py        # é€šç”¨è®­ç»ƒ
â”‚   â”œâ”€â”€ self_play.py            # ä¼ ç»Ÿè‡ªå¯¹å¼ˆ
â”‚   â”œâ”€â”€ self_play_generic.py    # é€šç”¨è‡ªå¯¹å¼ˆ
â”‚   â”œâ”€â”€ arena.py                # ELO è¯„æµ‹
â”‚   â””â”€â”€ test_integration.py     # é›†æˆæµ‹è¯•
â”‚
â””â”€â”€ docs/                        # æ–‡æ¡£
    â”œâ”€â”€ README_FRAMEWORK.md     # æ¶æ„è¯¦æƒ…
    â”œâ”€â”€ MIGRATION_GUIDE.md      # è¿ç§»æŒ‡å—
    â”œâ”€â”€ VERSION_COMPARISON.md   # ç‰ˆæœ¬å¯¹æ¯”
    â”œâ”€â”€ ARENA_GUIDE.md          # ç«æŠ€åœºä½¿ç”¨æŒ‡å—
    â””â”€â”€ CHANGELOG.md            # ç‰ˆæœ¬å†å²
```

### æ ¸å¿ƒç»„ä»¶

#### 1. æ¸¸æˆå¼•æ“ï¼ˆ`xq/state.py`ï¼‰

ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œå®ç°å®Œæ•´çš„è±¡æ£‹è§„åˆ™ï¼š

**ä¸»è¦ç‰¹æ€§ï¼š**
- ä½¿ç”¨æœ‰ç¬¦å·æ•´æ•°çš„ 9Ã—10 æ£‹ç›˜è¡¨ç¤º
- å¢é‡å¼èµ°æ³•åº”ç”¨ï¼Œæ”¯æŒæ’¤é”€
- Zobrist å“ˆå¸Œç”¨äºå¿«é€Ÿå±€é¢æŸ¥æ‰¾
- ä¼ªåˆæ³•å’Œåˆæ³•èµ°æ³•ç”Ÿæˆ
- å°†å†›æ£€æµ‹å’Œå°†æ­»åˆ¤å®š
- ä¸‰æ¬¡é‡å¤æ£€æµ‹
- ä¸­å›½è±¡æ£‹ç‰¹å®šè§„åˆ™ï¼š
  - å°†ã€å£«çš„ä¹å®«é™åˆ¶
  - å…µè¿‡æ²³åçš„èµ°æ³•
  - ç‚®çš„è·³åƒæœºåˆ¶
  - é©¬çš„è¹©è…¿æ£€æµ‹
  - å°†å¸…ç…§é¢è§„åˆ™
  - é•¿å°†/é•¿æ‰æ£€æµ‹

**æ€§èƒ½ï¼š**
- åˆå§‹å±€é¢çº¦ 44 ä¸ªåˆæ³•èµ°æ³•
- äºšæ¯«ç§’çº§èµ°æ³•ç”Ÿæˆ
- O(1) å±€é¢æŸ¥æ‰¾ï¼ˆZobrist å“ˆå¸Œï¼‰

#### 2. ç¥ç»ç½‘ç»œæ¶æ„

**è¾“å…¥è¡¨ç¤ºï¼š**
- 15 é€šé“ Ã— 10 è¡Œ Ã— 9 åˆ—
- é€šé“ï¼š14 ç§æ£‹å­ï¼ˆ7 ç§ Ã— 2 è‰²ï¼‰+ 1 è¡Œæ£‹æ–¹

**ç½‘ç»œæ¶æ„ï¼ˆé€šç”¨ç‰ˆï¼‰ï¼š**
```
è¾“å…¥ (15, 10, 9)
    â†“
Conv2d(3Ã—3) + BatchNorm + ReLU
    â†“
N Ã— æ®‹å·®å—
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç­–ç•¥å¤´      â”‚   ä»·å€¼å¤´     â”‚
â”‚              â”‚              â”‚
â”‚ Conv(1Ã—1)    â”‚  Conv(1Ã—1)   â”‚
â”‚ + BatchNorm  â”‚  + BatchNorm â”‚
â”‚ + Flatten    â”‚  + Flatten   â”‚
â”‚ + Linear     â”‚  + Linear    â”‚
â”‚              â”‚  + Tanh      â”‚
â”‚ (8100 ç»´)    â”‚  (æ ‡é‡)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®å‚æ•°ï¼š**
- `hidden_channels`: 64ï¼ˆé»˜è®¤ï¼‰ï¼Œ128ï¼ˆé«˜çº§ï¼‰
- `num_res_blocks`: 3ï¼ˆé»˜è®¤ï¼‰ï¼Œ7-10ï¼ˆé«˜çº§ï¼‰
- åŠ¨ä½œç©ºé—´ï¼š8100ï¼ˆ90Ã—90 èµ·ç‚¹-ç»ˆç‚¹ï¼‰
- ä»·å€¼èŒƒå›´ï¼š[-1, 1]ï¼ˆè¾“åˆ°èµ¢ï¼‰

**ç½‘ç»œç±»å‹ï¼š**
1. **ä¼ ç»Ÿ XQNet**ï¼šæ—  BatchNorm çš„ç®€å• CNN
2. **é€šç”¨ PolicyValueNet**ï¼šå¸¦ BatchNorm çš„ç°ä»£æ¶æ„

#### 3. æœç´¢ç®—æ³•

##### è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰

**ç®—æ³•ï¼š** PUCTï¼ˆé¢„æµ‹å™¨ + æ ‘çš„ç½®ä¿¡ä¸Šç•Œï¼‰

**å…¬å¼ï¼š**
$$ UCT = Q(s,a) + c\_puct Ã— P(s,a) Ã— \frac{\sqrt{N(s)}} {(1 + N(s,a))} $$

**ç‰¹æ€§ï¼š**
- Dirichlet å™ªå£°ç”¨äºæ¢ç´¢ï¼ˆÎ±=0.3ï¼‰
- æ¸©åº¦å‚æ•°æ§åˆ¶åŠ¨ä½œé€‰æ‹©
- è™šæ‹ŸæŸå¤±æ”¯æŒå¹¶è¡Œæœç´¢
- åˆæ³•èµ°æ³•æ©ç 

**å‚æ•°ï¼š**
- `num_simulations`: 100-800ï¼ˆé»˜è®¤ï¼š200ï¼‰
- `c_puct`: 1.5ï¼ˆæ¢ç´¢å¸¸æ•°ï¼‰
- `temperature`: 1.0ï¼ˆæ¢ç´¢ï¼‰â†’ 0.1ï¼ˆåˆ©ç”¨ï¼‰

##### Alpha-Beta æœç´¢

**ç‰¹æ€§ï¼š**
- Negamax æ¡†æ¶
- ä½¿ç”¨ Zobrist å“ˆå¸Œçš„ç½®æ¢è¡¨ï¼ˆTTï¼‰
- é™æ€æœç´¢ç¡®ä¿æˆ˜æœ¯ç¨³å®šæ€§
- èµ°æ³•æ’åºï¼š
  - MVV/LVAï¼ˆæœ€æœ‰ä»·å€¼å—å®³è€…/æœ€ä½ä»·å€¼æ”»å‡»è€…ï¼‰
  - å†å²å¯å‘
  - æ€æ‰‹èµ°æ³•

**å‚æ•°ï¼š**
- `depth`: 3-6ï¼ˆé»˜è®¤ï¼š3ï¼‰
- `tt_size_mb`: 256ï¼ˆç½®æ¢è¡¨å¤§å°ï¼‰

#### 4. è®­ç»ƒæµç¨‹

**è‡ªå¯¹å¼ˆè¿‡ç¨‹ï¼š**
```
1. ä»åˆå§‹å±€é¢å¼€å§‹
2. å¯¹äºæ¯æ­¥æ£‹ï¼š
   - è¿è¡Œ MCTSï¼ˆåŠ  Dirichlet å™ªå£°ï¼‰
   - åŸºäºè®¿é—®æ¬¡æ•°é‡‡æ ·åŠ¨ä½œ
   - åº”ç”¨èµ°æ³•
3. è®°å½•ï¼ˆçŠ¶æ€ã€ç­–ç•¥ã€ä»·å€¼ï¼‰ä¸‰å…ƒç»„
4. ç»§ç»­ç›´åˆ°æ¸¸æˆç»“æŸ
5. ä¸ºæ‰€æœ‰å±€é¢åˆ†é…æœ€ç»ˆç»“æœ
6. ä¿å­˜ä¸º JSONL æ ¼å¼
```

**è®­ç»ƒè¿‡ç¨‹ï¼š**
```
1. ä» JSONL åŠ è½½è‡ªå¯¹å¼ˆæ•°æ®
2. åˆ›å»º PyTorch DataLoader
3. å¯¹äºæ¯ä¸ª epochï¼š
   - å‰å‘ä¼ æ’­ï¼šé¢„æµ‹ç­–ç•¥å’Œä»·å€¼
   - æŸå¤± = CrossEntropy(ç­–ç•¥) + MSE(ä»·å€¼)
   - åå‘ä¼ æ’­å¹¶ä¼˜åŒ–
4. ä¿å­˜æ£€æŸ¥ç‚¹
```

**æ•°æ®æ ¼å¼ï¼ˆJSONLï¼‰ï¼š**
```json
{
  "game_id": 0,
  "result": 1,
  "records": [
    {
      "planes": [[...], ...],  // 15Ã—90 æ£‹ç›˜çŠ¶æ€
      "pi": {"123": 0.15, ...}, // ç­–ç•¥åˆ†å¸ƒ
      "z": 1.0                  // æ¸¸æˆç»“æœ
    }
  ],
  "timestamp": "2025-11-05T..."
}
```

### REST API å‚è€ƒ

åŸºç¡€ URLï¼š`http://127.0.0.1:8000/api`

#### æ¸¸æˆç®¡ç†

**åˆ›å»ºæ¸¸æˆ**
```
POST /games
Body: {
  "squares": [int] (å¯é€‰),
  "side_to_move": int (å¯é€‰)
}
Response: {
  "game_id": "uuid",
  "state": {...}
}
```

**è·å–æ¸¸æˆçŠ¶æ€**
```
GET /games/{game_id}
Response: {
  "board": [int],
  "side_to_move": int,
  "legal_moves": [...]
}
```

**èµ°æ£‹**
```
POST /games/{game_id}/move
Body: {
  "from_sq": int,
  "to_sq": int
}
```

**æ‚”æ£‹**
```
POST /games/{game_id}/undo
```

#### AI åŠŸèƒ½

**è·å–åˆæ³•èµ°æ³•**
```
GET /games/{game_id}/legal_moves
Response: [
  {"from": 81, "to": 72, "move_id": 123, ...}
]
```

**è·å–æœ€ä½³èµ°æ³•**
```
POST /games/{game_id}/best
Body: {
  "engine": "alphabeta" | "mcts" | "mcts_nn",
  "depth": int (Alpha-Beta ç”¨),
  "sims": int (MCTS ç”¨),
  "model_path": string (MCTS+NN ç”¨)
}
Response: {
  "best": {"from": 81, "to": 72},
  "score": float,
  "pi": {...}
}
```

**äººæœºå¯¹æˆ˜**
```
POST /games/{game_id}/human_ai
Body: {
  "human_move": "a0-a1",
  "engine": "mcts_nn",
  "sims": 200
}
Response: {
  "human": {...},
  "ai": {...},
  "state": {...}
}
```

#### æ¨¡å‹ç®¡ç†

**åˆ—å‡ºæ¨¡å‹**
```
GET /model/list?dir=models
Response: {
  "models": ["latest.pt", ...]
}
```

**è·å–æ¨¡å‹ä¿¡æ¯**
```
GET /model/info?model_path=models/latest.pt
Response: {
  "path": "...",
  "size_mb": 2.5,
  "parameters": 150000,
  "structure": [...]
}
```

**è·å–æ¨¡å‹æ¡†æ¶**
```
GET /model/framework
Response: {
  "loaded": true,
  "path": "models/latest.pt",
  "framework": "generic" | "legacy"
}
```

#### è®­ç»ƒä¸è¯„æµ‹

**è‡ªå¯¹å¼ˆ**
```
POST /selfplay
Body: {
  "engine": "mcts" | "mcts_nn",
  "games": 1,
  "sims": 200,
  "max_moves": 200
}
```

**ç«æŠ€åœºè¯„æµ‹**
```
POST /arena/run
Body: {
  "engine_a": "mcts_nn",
  "engine_b": "alphabeta",
  "model_a": "models/v1.pt",
  "params_a": {"sims": 200},
  "params_b": {"depth": 3},
  "n_games": 20
}
Response: {
  "elo_diff": 121.3,
  "win_rate": 0.675,
  "wins": 12,
  "draws": 3,
  "losses": 5
}
```

### å®‰è£…ä¸è®¾ç½®

#### æœ¬åœ°å®‰è£…

**å‰ç½®è¦æ±‚ï¼š**
```bash
# Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
python --version

# pip åŒ…ç®¡ç†å™¨
pip --version
```

**å®‰è£…ä¾èµ–ï¼š**
```bash
# æ ¸å¿ƒä¾èµ–
pip install torch torchvision  # PyTorch
pip install fastapi uvicorn    # API æœåŠ¡å™¨
pip install numpy              # æ•°å€¼è®¡ç®—

# å¯é€‰ï¼šGPU æ”¯æŒ
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

**å¿«é€Ÿå¼€å§‹ï¼š**

1. **å…‹éš†æˆ–ä¸‹è½½é¡¹ç›®**

2. **å¯åŠ¨ API æœåŠ¡å™¨ï¼š**
```bash
uvicorn api.server:app --host 127.0.0.1 --port 8000
```

3. **è®¿é—® Web ç•Œé¢ï¼š**
- ä¸»æ¸¸æˆï¼šhttp://127.0.0.1:8000/web/
- æ¨¡å‹æŸ¥çœ‹å™¨ï¼šhttp://127.0.0.1:8000/web/model.html
- ç«æŠ€åœºï¼šhttp://127.0.0.1:8000/web/arena.html

#### â˜ï¸ Azure äº‘éƒ¨ç½²

ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Œæ”¯æŒ GPU è®­ç»ƒï¼š

**å¿«é€Ÿéƒ¨ç½²ï¼ˆ10 åˆ†é’Ÿï¼‰ï¼š**
```bash
cd deployment/azure/scripts
bash deploy_to_azure.sh
```

**æ–‡æ¡£èµ„æºï¼š**
- ğŸ“– [Azure å¿«é€Ÿå¼€å§‹æŒ‡å—](deployment/azure/docs/AZURE_QUICKSTART.md) - ä»è¿™é‡Œå¼€å§‹ï¼
- ğŸ“– [å®Œæ•´éƒ¨ç½²æŒ‡å—](deployment/azure/docs/azure_deployment_guide.md) - è¯¦ç»†è¯´æ˜
- ğŸ“‹ [éƒ¨ç½²æ£€æŸ¥æ¸…å•](deployment/azure/docs/deployment_checklist.md) - åˆ†æ­¥æŒ‡å¯¼
- ğŸ’° æˆæœ¬ï¼šçº¦ $50/æœˆï¼ˆåŒ…å« GPU è®­ç»ƒï¼‰

**ç‰¹æ€§ï¼š**
- ä¸€é”®éƒ¨ç½²åˆ° Azure App Service
- Azure VM ä¸Šè‡ªåŠ¨åŒ– GPU è®­ç»ƒ
- Blob Storage å­˜å‚¨æ¨¡å‹å’Œæ•°æ®
- è‡ªåŠ¨æ‰©å±•å’Œç›‘æ§
- é¢„ç®—å‹å¥½ï¼ˆåŒ…å« $150/æœˆæ–¹æ¡ˆï¼‰

### ä½¿ç”¨ç¤ºä¾‹

#### ç¤ºä¾‹ 1ï¼šè‡ªå¯¹å¼ˆè®­ç»ƒ

```bash
# æ­¥éª¤ 1ï¼šç”Ÿæˆè‡ªå¯¹å¼ˆæ•°æ®
python scripts/self_play_generic.py \
    --game xiangqi \
    --games 50 \
    --sims 200 \
    --out data/selfplay_001.jsonl

# æ­¥éª¤ 2ï¼šè®­ç»ƒæ¨¡å‹
python scripts/train_generic.py \
    --game xiangqi \
    --data data/selfplay_001.jsonl \
    --model_out models/v1.pt \
    --epochs 10 \
    --batch_size 64 \
    --lr 0.001

# æ­¥éª¤ 3ï¼šä½¿ç”¨æ–°æ¨¡å‹é‡å¤
python scripts/self_play_generic.py \
    --game xiangqi \
    --games 50 \
    --sims 200 \
    --model models/v1.pt \
    --out data/selfplay_002.jsonl
```

#### ç¤ºä¾‹ 2ï¼šæ¨¡å‹è¯„æµ‹

```bash
# æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹
python scripts/arena.py \
    --engine-a mcts_nn --model-a models/v1.pt \
    --engine-b mcts_nn --model-b models/v2.pt \
    --games 20 \
    --output results.json

# ä¸ Alpha-Beta åŸºå‡†æµ‹è¯•
python scripts/arena.py \
    --engine-a mcts_nn --model-a models/latest.pt \
    --engine-b alphabeta --depth-b 3 \
    --games 20
```

#### ç¤ºä¾‹ 3ï¼šé€šè¿‡ API è¿›è¡Œäººæœºå¯¹æˆ˜

```python
import requests

# åˆ›å»ºæ¸¸æˆ
response = requests.post('http://127.0.0.1:8000/api/games')
game_id = response.json()['game_id']

# èµ°æ£‹å¹¶è·å– AI å›åº”
response = requests.post(
    f'http://127.0.0.1:8000/api/games/{game_id}/human_ai',
    json={
        'human_move': 'b9-c7',  # è·³é©¬
        'engine': 'mcts_nn',
        'sims': 200
    }
)

ai_move = response.json()['ai']
print(f"AI èµ°æ£‹ï¼š{ai_move['from_coord']} â†’ {ai_move['to_coord']}")
```

### å…³é”®ç®—æ³•

#### 1. èµ°æ³•ç”Ÿæˆï¼ˆä¼ªä»£ç ï¼‰

```python
def generate_legal_moves(state):
    pseudo_legal = []
    
    # éå†å½“å‰ç©å®¶çš„æ¯ä¸ªæ£‹å­
    for square, piece in enumerate(state.board):
        if piece.color != state.side_to_move:
            continue
            
        # ç”Ÿæˆç‰¹å®šæ£‹å­çš„èµ°æ³•
        if piece.type == PAWN:
            moves = generate_pawn_moves(square, piece)
        elif piece.type == CANNON:
            moves = generate_cannon_moves(square, piece)
        # ... å…¶ä»–æ£‹å­
        
        pseudo_legal.extend(moves)
    
    # è¿‡æ»¤éæ³•èµ°æ³•ï¼ˆå°†å†›ï¼‰
    legal = []
    for move in pseudo_legal:
        state.apply_move(move)
        if not state.is_in_check(opponent):
            legal.append(move)
        state.undo_move()
    
    return legal
```

#### 2. MCTS é€‰æ‹©ï¼ˆä¼ªä»£ç ï¼‰

```python
def select(node, state):
    while not node.is_leaf():
        # PUCT å…¬å¼
        best_action = None
        best_value = -inf
        
        for action, child in node.children.items():
            q = child.value()  # å¹³å‡ä»·å€¼
            u = c_puct * child.prior * sqrt(node.visits) / (1 + child.visits)
            value = q + u
            
            if value > best_value:
                best_value = value
                best_action = action
        
        state.apply_move(best_action)
        node = node.children[best_action]
    
    return node, state
```

#### 3. Alpha-Beta æœç´¢ï¼ˆä¼ªä»£ç ï¼‰

```python
def alphabeta(state, depth, alpha, beta):
    # ç»ˆæ­¢æ¡ä»¶
    if depth == 0 or state.is_terminal():
        return evaluate(state)
    
    # æŸ¥è¯¢ç½®æ¢è¡¨
    tt_entry = tt.lookup(state.hash)
    if tt_entry and tt_entry.depth >= depth:
        return tt_entry.score
    
    # èµ°æ³•æ’åº
    moves = state.generate_legal_moves()
    moves = order_moves(moves, state)  # MVV/LVAã€å†å²ã€æ€æ‰‹
    
    best_score = -infinity
    for move in moves:
        state.apply_move(move)
        score = -alphabeta(state, depth-1, -beta, -alpha)
        state.undo_move()
        
        best_score = max(best_score, score)
        alpha = max(alpha, score)
        
        if alpha >= beta:  # Beta å‰ªæ
            record_killer(move)
            break
    
    # å­˜å…¥ç½®æ¢è¡¨
    tt.store(state.hash, depth, best_score)
    
    return best_score
```

### æ€§èƒ½åŸºå‡†

**ç¡¬ä»¶ï¼š** Intel i7-10700K, 16GB RAM, ä»… CPU

| æ“ä½œ | æ—¶é—´ | å¤‡æ³¨ |
|------|------|------|
| èµ°æ³•ç”Ÿæˆ | < 1 ms | åˆå§‹å±€é¢çº¦ 44 ä¸ªåˆæ³•èµ°æ³• |
| å±€é¢è¯„ä¼° | < 0.1 ms | å­åŠ› + åŸºæœ¬å¯å‘ |
| Alpha-Betaï¼ˆæ·±åº¦=3ï¼‰ | ~2-5 ç§’ | å¸¦ TT å’Œèµ°æ³•æ’åº |
| MCTSï¼ˆ200 æ¨¡æ‹Ÿï¼‰ | ~5-10 ç§’ | ç®€å•ç­–ç•¥ |
| MCTS+NNï¼ˆ200 æ¨¡æ‹Ÿï¼‰ | ~15-30 ç§’ | ç¥ç»ç½‘ç»œ |
| NN æ¨ç† | ~10 ms | å•ä¸ªå±€é¢ï¼ŒCPU |
| è‡ªå¯¹å¼ˆå¯¹å±€ | ~2-5 åˆ†é’Ÿ | MCTSï¼Œ200 æ¨¡æ‹Ÿï¼Œçº¦ 100 æ­¥ |

### æµ‹è¯•

è¿è¡Œé›†æˆæµ‹è¯•ï¼š
```bash
python scripts/test_integration.py
```

**æµ‹è¯•è¦†ç›–ï¼š**
- âœ… å¯¼å…¥æµ‹è¯•ï¼ˆä¼ ç»Ÿ + é€šç”¨ï¼‰
- âœ… ä¼ ç»Ÿæ¡†æ¶ï¼ˆXQNetã€çŠ¶æ€ã€èµ°æ³•ï¼‰
- âœ… é€šç”¨æ¡†æ¶ï¼ˆGameInterfaceã€MCTSã€Trainerï¼‰
- âœ… æ¨¡å‹å…¼å®¹æ€§ï¼ˆä¿å­˜/åŠ è½½ï¼‰
- âœ… GameInterface å®ç°

### æ‰©å±•åˆ°å…¶ä»–æ¸¸æˆ

é€šç”¨æ¡†æ¶ä½¿æ·»åŠ æ–°æ¸¸æˆå˜å¾—ç®€å•ï¼š

```python
# games/go/game_adapter.py
from alphazero import GameInterface

class GoGame(GameInterface):
    def get_initial_state(self):
        return empty_19x19_board()
    
    def get_action_size(self):
        return 19 * 19 + 1  # +1 è¡¨ç¤º pass
    
    def get_observation_shape(self):
        return (17, 19, 19)  # 8 å†å² Ã— 2 è‰² + è¡Œæ£‹æ–¹
    
    # å®ç°å…¶ä»–æŠ½è±¡æ–¹æ³•...

# ä¸ç°æœ‰æ¡†æ¶é…åˆä½¿ç”¨
game = GoGame()
model = PolicyValueNet(NetworkConfig(
    input_channels=17,
    board_height=19,
    board_width=19,
    action_size=362
))
```

### æœªæ¥å¢å¼º

- [ ] GPU åŠ é€Ÿå’Œæ‰¹é‡æ¨ç†
- [ ] è·¨å¤šå°æœºå™¨çš„åˆ†å¸ƒå¼è‡ªå¯¹å¼ˆ
- [ ] å¼€å±€åº“é›†æˆ
- [ ] æ®‹å±€æ•°æ®åº“
- [ ] å¸¦ SPRT çš„ ELO è¯„çº§ç³»ç»Ÿ
- [ ] å¤šæ¸¸æˆè®­ç»ƒï¼ˆè¿ç§»å­¦ä¹ ï¼‰
- [ ] ç§»åŠ¨åº”ç”¨ï¼ˆReact Nativeï¼‰
- [ ] ç‘å£«åˆ¶é…å¯¹çš„é”¦æ ‡èµ›æ¨¡å¼

### æŠ€æœ¯äº®ç‚¹ï¼ˆæ±‚èŒç”¨ï¼‰

**å±•ç¤ºçš„æŠ€èƒ½ï¼š**

1. **æ·±åº¦å­¦ä¹ **
   - PyTorch ç¥ç»ç½‘ç»œè®¾è®¡å’Œè®­ç»ƒ
   - ç­–ç•¥-ä»·å€¼ç½‘ç»œæ¶æ„
   - æ®‹å·®è¿æ¥å’Œ BatchNorm

2. **å¼ºåŒ–å­¦ä¹ **
   - AlphaZero ç®—æ³•å®ç°
   - è’™ç‰¹å¡æ´›æ ‘æœç´¢
   - è‡ªå¯¹å¼ˆè®­ç»ƒæµç¨‹

3. **è½¯ä»¶å·¥ç¨‹**
   - å¸¦æŠ½è±¡æ¥å£çš„æ¸…æ™°æ¶æ„
   - æ¨¡å—åŒ–è®¾è®¡ä¾¿äºæ‰©å±•
   - å…¨é¢çš„æ–‡æ¡£

4. **åç«¯å¼€å‘**
   - ä½¿ç”¨ FastAPI çš„ RESTful API
   - å¼‚æ­¥è¯·æ±‚å¤„ç†
   - çŠ¶æ€ç®¡ç†

5. **å‰ç«¯å¼€å‘**
   - React ç»„ä»¶å’Œ hooks
   - äº¤äº’å¼æ¸¸æˆç•Œé¢
   - æ•°æ®å¯è§†åŒ–

6. **ç®—æ³•**
   - åšå¼ˆæ ‘æœç´¢ï¼ˆAlpha-Betaã€MCTSï¼‰
   - å¯å‘å¼ä¼˜åŒ–ï¼ˆTTã€QSã€èµ°æ³•æ’åºï¼‰
   - å¤æ‚è§„åˆ™æ‰§è¡Œ

7. **æµ‹è¯•ä¸è´¨é‡**
   - é›†æˆæµ‹è¯•å¥—ä»¶
   - æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶å’Œè¯„ä¼°
   - æ€§èƒ½åŸºå‡†æµ‹è¯•

### è®¸å¯è¯

æœ¬é¡¹ç›®ç”¨äºæ•™è‚²å’Œä½œå“é›†ç›®çš„ã€‚

### è”ç³»æ–¹å¼

å¦‚æœ‰å·¥ä½œæœºä¼šæˆ–æŠ€æœ¯è®¨è®ºï¼Œè¯·é€šè¿‡ GitHub è”ç³»ã€‚

---

**é¡¹ç›®å®Œæˆåº¦ï¼š100%**  
**ä»£ç è¡Œæ•°ï¼š~5000+ è¡Œ**  
**æ–‡æ¡£ï¼šå®Œæ•´çš„ä¸­è‹±æ–‡æ–‡æ¡£**  
**æµ‹è¯•è¦†ç›–ï¼šé›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡**


